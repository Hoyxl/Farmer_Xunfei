# 笔记一

## 深度残差网络（ResNet）

​	凭经验来说，模型层数越深应当具有越强的学习能力，但是，随着模型层数的增多，模型反而出现了退化现象。ResNet是解决这一现象的一个神经网络模型。

​	基于模型的这一反常现象，提出一种假设：模型的增加的层数仅仅是对之前层简单的映射，这样模型也应该仅仅是原地踏步而不应该出现退化现象。因此应该是模型结构本身训练过程出现了问题。

​	残差学习应运而生，对于一个堆积层结构：当输入为$x$时学习到的特征为$H(x)$，我们希望网络能学习到残差为$F(x)=H(x)-x$，这样其原始的学习特征是$F(x)+x$。

​	残差学习单位可以表示为：
$$
y_l=h(x_l)+F(x_l,W_l)\\
x_{l+1}=f(y_l)
$$
​	其中$x_l$和$x_{l+1}$表示第$l$个残差单元的输入和输出(每个残差单元一般包含多层结构)。$F$是残差函数，表示学到的残差，而$h(x_l)=x_l$表示恒等映射，$f$是$ReLU$激活函数。因此，浅层$l$到深层$L$的学习特征为：
$$
x_L=x_l+\sum_i^{L-1}F(x_i, W_i)
$$
​	根据链式规则，反向过程的梯度为：
$$
\frac{\part{loss}}{\part{x_l}}=\frac{\part{loss}}{\part{x_L}}\cdot\frac{\part{x_L}}{\part{x_l}}=\frac{\part{loss}}{\part{x_L}}\cdot\left(1+\frac{\part}{\part{x_l}}\sum_{i=l}^{L-1}F(x_i,W_i)\right)
$$
​	式子的第一个因子$\frac{\part{loss}}{\part{x_l}}$表示的损失函数$L$到达的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。上面的推导并不是严格的证明。

​	$ResNet$是在$VGG19$的基础上修改而来的，通过短路机制加入了残差单元。变化主要体现在$ResNet$直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。$ResNet$的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。

![](https://pic4.zhimg.com/80/v2-252e6d9979a2a91c2d3033b9b73eb69f_720w.webp)

## Macro F1 Score

​	macro F1 score，即宏平均F1分数，是一种用于评估多分类问题的指标，它是每个类别的F1分数的平均值。F1分数是精确率和召回率的加权平均，它可以反映模型对不同类别的识别能力。macro F1 score可以用来衡量模型对所有类别的整体性能，它不受数据不平衡的影响，但容易受到识别性高的类别的影响。

计算公式如下：
$$
\text{Macro F1 Score} = \frac{1}{n}\sum_{i=1}^{n}\text{F1 Score}_i
$$
## 模型改进思路

- 适当增加训练$epoch$，监控$val\ loss$，当其具有上升趋势时，认为可能已经过拟合，可以提前停下训练过程。
- 使用不同的模型进行投票，对训练过程中的一些模型，也让其参与最后结果的决定投票，以得到一个尽可能稳定的结果。
- 使用$k$折验证法，提高稳定性。
- 以后可能得改进方向：尝试调整其他参数，采用贝叶斯优化和其他数据增强方法，调参以获得最优参数等。